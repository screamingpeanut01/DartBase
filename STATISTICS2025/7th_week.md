# 7th_week

# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.

## Statistics_7th_TIL

# 13. 머신러닝 분석 방법론

## 13.1 선형 회귀분석과 Elastic Net (예측모델)

### 13.1.1 회귀분석의 기원과 원리

- **기본 원리**: 독립변수 X들의 조건을 고려하여 종속변수 Y의 평균값을 예측
    - 정보가 없을 시 전체 평균으로 예측
    - 추가 정보(예: 성별, 몸무게)가 주어지면 해당 집단의 평균을 사용하여 더 정확한 예측
    - 몸무게를 세분화하면 각 단위 집단의 평균키를 통과하는 직선(회귀선)을 그릴 수 있으며, 이때 발생하는 오차를 최소화
- **회귀식의 구성**: y=β0+β1X1+⋅⋅⋅+βnXn+ϵ
    - y: 예측 값 (종속변수)
    - β0: 절편
    - β1∼βn: 각 독립변수의 계수 (영향력)
    - X1∼Xn: 각 독립변수 값
    - ϵ: 잔차 (모델로 설명되지 않는 부분)
- **최적의 회귀선**: 최소제곱추정법(Least Squares Estimation, LSE)을 사용하여 잔차제곱합(RSS)을 최소화하는 절편과 기울기
    
    대우도추정법(Maximum Likelihood Estimation, MLE)과 동일한 원리
    
- **회귀분석의 종류**:
    - 단순 회귀분석: 독립변수가 하나.
    - 다중 회귀분석: 독립변수가 두 개 이상. 다중공선성 검사가 필요합니다. (VIF 값 확인, 차원축소 등으로 해결)
- **기본 조건**:
    - 잔차의 정규성: 잔차가 정규분포를 따라야  (히스토그램, Q-Q Plot, 샤피로-윌크 검정 등으로 확인)
    - 잔차의 등분산성: 잔차의 분산은 독립 변숫값과 상관없이 일정해야
    - 독립성: 독립변수들 간 상관관계가 없어야
    - 선형성: 독립변수 값 변화에 따른 종속변수 값 변화가 일정해야
- **비선형 관계 처리**:
    - 변수 구간화 후 더미변수로 변환.
    - 독립/종속변수에 로그함수 적용.

### 13.1.2 다항 회귀 (Polynomial regression)

- 독립변수와 종속변수가 곡선형 관계일 때, 변수에 각 특성의 제곱항 등을 추가하여 회귀선을 곡선형으로 변환하는 모델
- 차수가 커질수록 편향은 감소하나 변동성이 증가하여 과적합을 유발

### 회귀분석 모델 활용 및 평가

- **활용 분야**: 고객 구매 패턴 예측, 제품 수요량, 마케팅 효과, 웹사이트 접속량 예측 등.
- **데이터 분리**: 학습셋, 검증셋, 테스트셋으로 나누어 예측력 확인 및 모델 튜닝.
- **기본 가설**: 귀무가설 (모든 회귀계수=0), 대립가설 (적어도 하나의 회귀계수≠0).
- **결과 해석 (표 13.1 참조)**:
    - Parameter Estimate (계수): 독립변수 1단위 변화 시 종속변수 변화량. Intercept (절편)는 기본값.
    - Standard Error (표준오차): 예측값과 실제값 차이.
    - T Value: 계수를 표준오차로 나눈 값, 독립변수와 종속변수 간 선형관계 강도. (절댓값 1.98 이상이면 유의미, 관측치 100개 기준)
    - P Value (유의확률): T Value와 관측치 수에 의해 결정. (일반적으로 0.05 이하 시 유의)
    - Tolerance & VIF: 다중공선성 판단 지표. (VIF = 1/Tolerance)
- **변수 선택 알고리즘**:
    - 전진 선택법 (Forward Selection): 절편만 있는 모델에서 유의미한 변수 순차적 추가.
    - 후진 제거법 (Backward Elimination): 모든 변수 포함 상태에서 유의미하지 않은 변수 순차적 제거.
    - 단계적 선택법 (Stepwise Selection): 전진/후진법 결합. 선택된 변수 모델의 잔차와 나머지 변수 간 상관도를 구해 변수 선택.
    - 기타: LARS, 유전 알고리즘.

### 13.1.3 Ridge와 Lasso 그리고 Elastic Net

최근에는 변수 계수에 가중치를 주어 편향을 허용함으로써 예측 정밀도를 향상시키는 Ridge, Lasso, Elastic Net을 많이 사용

- **Ridge 회귀**: 전체 변수를 유지하며 계수 크기를 조정 (L2-norm). λ 값으로 정규화 수준 조정. λ=0이면 선형회귀와 동일, λ가 크면 회귀선은 수평에 가까워짐. 다중공선성 방지 및 모델 설명력 최대화.
- **Lasso 회귀**: 중요한 몇 개 변수만 선택, 나머지 변수 계수를 0으로 만듦 (L1-norm). α 값으로 정규화 강도 조정. 모델 단순화 및 해석 용이.
- **Elastic Net**: Ridge와 Lasso를 결합한 모델. 혼합비율(r)로 Ridge(r≈0)와 Lasso(r≈1)의 비중 조절. 중요 변수 선별 시 Ridge 비율 증가, 모든 변수 사용 시 Lasso 비율 증가.
- **회귀분석 모델 자체 평가 (표 13.2 참조)**:
    - 모델 유의도 (F-statistic의 P value): 0.05보다 작으면 유의미.
    - R2 (결정계수): 모델의 설명력. (예: 0.773은 77.3% 설명)
    - Adj R2 (수정된 결정계수): 독립변수 개수 증가에 따른 R2 증가 보정.
    - P value와 R2 값에 따른 모델 튜닝 전략 (표 13.3 참조).

## 13.2 로지스틱 회귀분석 (분류 모델)

선형회귀분석과 유사하나 종속변수가 질적척도(범주형)인 분류 모델

- **기본 모형**: 이항 로지스틱 회귀 (0 또는 1 예측, 예: 구매/미구매).
- **원리**: 종속변수를 1이 될 확률로 변환 (오즈, 로짓 변환)하여 S자 곡선 형태로 예측.
    - 오즈 (Odds): Odds=1−P(event occurring)P(event occurring)
    - 로짓 변환: 오즈에 로그를 취한 후 시그모이드 함수 적용.
    P=1+e(β0+βX)e(β0+βX)=1+e−(β0+βX)1
- **분류 기준**: 일반적으로 확률 0.5, 상황에 따라 조절 가능.
- **다항 로지스틱 회귀분석**: 종속변수 범주가 3개 이상일 때, 각 범주마다 이항 로지스틱 시행 (K-1개의 식이 필요).
- **결과 해석 (표 13.4 참조)**:
    - 계수(Coefficient), 유의도.
    - 오즈비 (Odds Ratio): 독립변수 1단위 증가 시 종속변수가 1일 확률 배율.
    - R2 값: Tjur의 식별계수 등 다양한 방법 사용.
    - 실제 분류 정확도(ROC Curve, Confusion matrix 등)가 더 중요.

## 13.3 의사결정나무와 랜덤 포레스트 (예측/분류 모델)

나무 구조를 통해 데이터를 분리하며 예측 조건을 만드는 분석 기법. (스무고개 놀이와 유사)

### 13.3.1 분류나무와 회귀나무

- **분류나무 (Classification tree)**: 명목형 종속변수 분류.
    - 원리: 독립변수 조건을 통해 불순도(Impurity)를 낮추고 순도(Homogeneity)를 높이는 방향으로 분기.
    - 불순도 지표: 지니 계수 (Gini index), 엔트로피 (Entropy). 정보 획득량 (Information gain) 최대화.
        - 지니 계수: 1−∑(Pi)2.
        - 엔트로피: −∑Pilog2(Pi).
    - 알고리즘 종류 (표 13.5 참조): ID3, C4.5 (엔트로피), CHAID (카이제곱/F-통계량), CART (지니계수/분산감소량).
    - 노드 종류: 뿌리(Root), 부모(Parent), 자식(Child), 중간(Internal), 끝(Terminal/Leaf), 깊이(Depth).
- **회귀나무 (Regression tree)**: 연속형 종속변수 예측.
    - 원리: 잔차제곱합(RSS) 최소화 또는 분산 감소량 최대화, F-value 최대화 기준으로 분기. 끝 노드의 평균값으로 예측. 비선형성에 강함.

### 13.3.2 의사결정나무 모델의 장단점

- **장점**: 직관적 해석, 쉬운 구현, 비선형 데이터 처리, 데이터 전처리 요구사항 적음.
- **단점**:
    - 학습 데이터에 없는 명목형 값 예측 불가.
    - 연속형 변수의 예측 범위가 학습 데이터 범위로 한정.
    - 과적합(Overfitting) 가능성 높음.

### 13.3.3 의사결정나무 모델의 과적합 방지를 위한 방법

- 가지치기 (Pruning): 너무 세밀하게 분기된 가지를 통합. 검증 데이터의 오분류율이 최소가 되는 지점까지.
- 정보 획득량 임계값 설정.
- 한 노드의 최소 데이터 수 제한.
- 노드의 최대 깊이 제한.

### 13.3.4 랜덤 포레스트

의사결정나무의 과적합 문제를 보완하기 위해 여러 개의 의사결정나무를 만들어 학습하는 앙상블 학습(Ensemble learning) 기법.

- **원리**:
    1. 학습 데이터셋에서 부트스트랩(Bootstrap) 샘플링 (중복 허용).
    2. 독립변수 무작위 선택 (일반적으로 전체 변수 개수의 제곱근).
    3. 추출된 데이터와 변수로 의사결정나무 학습.
    4. 1~3단계 k번 반복.
    5. k개 나무의 예측값 종합 (분류: 다수결 투표(Voting), 회귀: 평균(Average)). 이 과정을 배깅(Bagging)이라 함.
- **부스팅 (Boosting)**: 배깅과 유사하나, 순차적으로 모델을 학습하며 정답/오답에 가중치를 부여. 더 정밀하나 과적합 위험, 이상치 취약.

## 13.4 선형 판별분석과 이차 판별분석 (분류 모델)

로지스틱 회귀분석처럼 질적 종속변수를 분류. 통계 기반 분류 모델. 로지스틱 회귀보다 적은 데이터로 유사 성능 가능, 독립변수 정규분포 아니어도 활용 가능.

- **종류**:
    - 일반 판별분석 (두 집단): 종속변수 범주 2개.
    - 다중 판별분석: 종속변수 범주 3개 이상.
    - 선형 판별분석 (LDA), 이차 판별분석 (QDA): 결정경계선 산출 방식에 따름.

### 13.4.1 선형 판별분석 (LDA)

분류 및 지도학습 기반 차원축소에 사용 (PCA보다 성능 우수).

- **원리**: 집단 내 분산 대비 집단 간 분산 차이를 최대화하는 선형 판별함수 Z=a0+a1X1+⋅⋅⋅+akXk 도출. 관측치별 분류점수를 계산하여 가장 큰 점수를 갖는 범주로 분류.
- **조건**: 데이터 정규분포, 범주 간 동일 공분산 행렬, 독립변수 상호 독립. (실무에서 완벽 충족 어려움)
- **결과 해석 (표 13.6 참조)**:
    - 판별상수(Constant)와 판별계수(Coefficient)로 분류점수 계산.
    - 오분류 비율로 정확도 평가.
    - 고유값 (Eigenvalue): 설명력 지표 (일반적으로 4.0 이상이면 유의미).
- 변수 선택 방법 (전진, 후진, 단계적) 사용 가능.

### 13.4.2 이차 판별분석 (QDA)

LDA가 공분산 구조가 다른 범주 데이터를 잘 분류 못 하는 단점 보완. 범주 간 공분산 구조 다를 때, 비선형 분류 가능. 독립변수 많을 시 연산량 증가.

- **원리**: 변수에 제곱항 추가, 곡선 형태 결정경계선. 오분류 기대비용(ECM) 최소화. 다변량 가우시안 분포의 로그-우도 함수 사용.

## 13.5 서포트벡터머신 (분류 모델)

범주를 나누는 최적의 구분선(결정경계선)을 찾아 범주를 예측하는 모델. 비선형 데이터에 강하고 과적합 경향 적음. (기본은 이진 분류)

- **원리**: 결정경계선 양쪽의 마진(Margin)을 최대화. 마진과 맞닿는 관측치인 서포트 벡터(Support vector)로 결정경계선 정의.
    - 데이터 정규화/표준화 필수.
    - 결정경계선: W⋅X+b=0. 마진 경계는 W⋅X+b=1과 W⋅X+b=−1.
    - 마진 거리: ∣∣w∣∣22.
- **소프트 마진 (Soft margin)**: 이상치를 일부 허용하여 과적합 방지 (하드 마진은 불허).
    - C 매개변수: 이상치 허용 정도 조절 (낮을수록 많이 허용).
- **커널 기법 (Kernel trick)**: 선형 분리 불가능 데이터를 고차원 공간으로 확장하여 분리.
    - 가우시안 RBF 커널: 많이 사용. 원점과의 거리(Z축) 추가 등으로 차원 확장.
    - Gamma 매개변수: 관측치 영향력 행사 거리 조절 (클수록 짧아짐, 가우시안 분포 표준편차 조절).
- C와 Gamma 값 조정을 통해 모델 복잡도 최적화.

## 13.6 KNN (분류, 예측 모델)

K-근접이웃(K-Nearest Neighbors)은 쉽고 직관적인 분류 및 회귀 예측 모델. 별도 학습 과정 없이 저장된 학습 데이터와 예측 데이터를 직접 대조 (메모리 기반 학습).

- **원리**: 공간상 가장 가까운 K개의 이웃 관측치의 범주(분류)나 값의 평균(회귀)으로 예측.
    - 1NN: 가장 가까운 1개 이웃으로 분류.
    - KNN: 가장 가까운 K개 이웃 중 다수결(분류) 또는 평균(회귀)으로 결정.
- **K 값 설정**:
    - 이진 분류 시 홀수 K 권장 (동률 방지).
    - 너무 작으면 이상치에 민감, 너무 크면 관련 적은 데이터 영향.
    - 교차검증(Cross validation)으로 오분류율이 가장 낮은 K 탐색.
- **거리 가중치**: 거리에 따라 가중치 부여 가능 (예: 1/d, 1/(1+d2), e−d).
- **거리 계산**: 유클리드 거리 사용. 데이터 정규화/표준화 필수.
- **장점**: 쉽고 직관적.
- **단점**: 변수/데이터 양 많을 시 연산량 증가, 특정 범주 편향 시 분류 어려움.

## 13.7 시계열 분석 (예측모델)

시간 흐름에 따른 관측치 변화를 순차적으로 데이터화하여 현황 모니터링 및 미래 예측. (주가 전망, 수요 예측 등) 예측 모델 외 분류 모델도 포함 가능하나, 여기서는 전통적 예측 모델 기준. (RNN, LSTM도 해당)

- **목적**: 탐색(패턴, 추세, 계절성, 인과관계 규명), 예측(미래 값 예측/분류).
- **시계열 구성요소 (**Xt=St+at**)**: 수준(Level), 추세(Trend), 순환성(Cycle), 계절성(Seasonality), 잡음(Noise).
- **시계열 분해**: 위 요소들을 나누는 것 (SEATS, STL 방법, Python `seasonal_decompose`).

### 13.7.1 회귀 기반 시계열 분석

예측 시점 값이 종속변수, 시점 해당 요소(요일, 월 등 더미변수)가 독립변수.
Yt=β0+β1X1t+⋅⋅⋅+βpXpt+wt

- **추세 반영**: 기준 시점으로부터 경과일을 독립변수로 사용.
- **비선형 추세**: 다항회귀 (Yt=β0+β1X1t+β2X2t2+wt) 또는 로그 변환.
- **외부 요소 반영**: 할인행사 여부, 공휴일, 날씨, 경제지표 등을 독립변수로 추가.
- **자기회귀(Autoregression) 요소**: 과거 시점(전월, 전년 동월 등)의 값을 독립변수로 추가.
    - ACF (Autocorrelation function): 시계열 데이터 주기성 및 특정 시차 영향력 확인.
    - PACF (Partial autocorrelation function): 다른 시점 영향 배제한 순수 영향력 척도.

### 13.7.2 ARIMA 모델

AR (자기회귀) + I (누적/차분) + MA (이동평균).

- **정상성(Stationarity)**: 모든 시점에 일정한 평균을 갖도록 변환 필요.
    - 차분(Difference): 현재값 - 이전값 (Yt−Yt−1), 추세 제거.
        - 일반 차분, 계절 차분 (Yt−Yt−s), 고차 차분.
    - 변환(Transformation): 로그, 루트 등으로 분산 안정화.
- **AR(p) 모델 (Autoregressive model)**: 현재값이 과거 p개의 값에 의존. (예: Yt=ϕ1Xt−1+wt)
- **MA(q) 모델 (Moving average model)**: 현재값이 과거 q개의 예측 오차에 의존. (예: Yt=μ+et+β1Xt−1) (여기서 $X_{t-1}$은 $e_{t-1}$을 의미하는 것으로 보임)
- **ARMA(p,q) 모델**: AR과 MA 모델 결합.
- **ARIMA(p,d,q) 모델**: 비정상 시계열을 d회 차분하여 정상화한 후 ARMA(p,q) 적용.
    - p: AR 차수, d: 차분 횟수, q: MA 차수.
- **분석 절차**: 시각화/ACF로 정상성 확인 -> 차분(d 결정) -> ACF/PACF로 p,q 결정 -> ARIMA 모델 적합.
- **평가**: RMSE, MAE, MAPE 등. 슬라이딩 윈도우로 검증 데이터 증폭 가능.

## 13.8 k-means 클러스터링 (군집 모델)

미리 정답(레이블) 없이 데이터 특성/구조를 발견하는 비지도학습. (KNN은 지도학습) 고객 세그먼트 분석과 유사.

- **원리**:
    1. K (군집 수)개의 중심점(Centroid)을 임의로 설정.
    2. 각 관측치를 가장 가까운 중심점에 할당.
    3. 각 군집 내 관측치들의 평균 위치로 중심점 이동.
    4. 중심점이 더 이상 이동하지 않을 때까지 2-3단계 반복.
    - 데이터 표준화/정규화 필수.
- **Local Minimum 문제**: 초기 중심점 위치에 따라 최적이 아닌 군집 생성 가능.
    - 해결: k-means++ 등 다양한 초기 중심점 선정 방법 사용.
- **적정 K 수 선정**:
    - 비즈니스 도메인 지식.
    - 엘보우 방법 (Elbow method): 군집 내 거리 합(Inertia)이 급감하는 지점의 K.
    - 실루엣 계수 (Silhouette coefficient): 군집 내 응집도와 군집 간 분리도를 나타내는 지표 (-1 ~ 1, 높을수록 좋음).
- **k-means 한계**: 구형(globular)이 아닌 복잡한 형태의 군집(예: 동심원) 분류 어려움.
- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: 밀도 기반 클러스터링, K 수 지정 불필요.
    - 매개변수: epsilon (이웃으로 간주할 거리), minPts (군집을 이루는 최소 관측치 수).
    - 장점: 임의 형태 군집 발견, 이상치 식별.
    - 단점: 연산량 많음 (특히 변수 많을 시), 적절한 파라미터 설정 어려움.
- **군집 결과 해석**: 각 군집의 특성(평균값 등)을 파악하여 군집 이름 부여 및 비즈니스 전략 수립.

## 13.9 연관규칙과 협업 필터링 (추천 모델)

과거부터 사용된 추천 시스템은 편집자 추천, 단순 집계(인기 아이템), 개인 맞춤형으로 구분됩니다. 여기서는 개인 맞춤형 추천 시스템을 다룹니다. (예: 아마존, 넷플릭스)

- **핵심**: 특정 상황에 특정 고객이 관심 가질 만한 아이템/콘텐츠 추천.
- **대표 방법**: 연관규칙, 협업 필터링, 콘텐츠 기반 필터링.

### 13.9.1 연관규칙 (Association rule)

"A 제품 구매 시 B 제품도 구매할 확률" (A → B)을 도출 (장바구니 분석).

- **알고리즘**: Apriori, FP-Growth, DHP 등. (Apriori 중심으로 설명)
- **용어**: 조건절 (Antecedent, IF), 결과절 (Consequent, THEN).
- **핵심 지표**:
    - **지지도 (Support)**: P(A∩B)=Nn(A∩B). 아이템 A와 B를 동시에 포함하는 거래 비율.
    - **신뢰도 (Confidence)**: P(B∣A)=n(A)n(A∩B). A 구매 시 B도 구매할 조건부 확률 (비대칭적).
    - **향상도 (Lift)**: Support(B)Confidence(A→B). A와 B가 독립적일 때 대비 함께 판매될 비율 (대칭적).
        - Lift = 1: 독립적. Lift > 1: 양의 연관성. Lift < 1: 음의 연관성.
- **활용**: 1단계-지지도/신뢰도 기준으로 필터링, 2단계-향상도 내림차순 정렬.
- **보완 지표**:
    - IS 측도: Lift(A→B)×Support(A,B).
    - 교차지지도: max(Support(ik))min(Support(ik)). 지지도 편차 확인용 보조지표.
- **Apriori 알고리즘**: 최소 지지도(min_support)를 설정하여 해당 기준 미달 조합 및 그 조합을 포함하는 모든 상위 조합을 연산에서 제외(pruning)하여 계산 효율성 증대.

### 13.9.2 콘텐츠 기반 필터링과 협업 필터링

- **콘텐츠 기반 필터링 (Content-Based Filtering)**: 아이템의 속성(메타 정보: 색상, 장르, 배우 등)을 활용하여 사용자 선호 아이템과 유사한 속성의 아이템 추천.
    - 장점: 정형화된 데이터로 유사 아이템 추천 용이.
    - 단점: 모든 아이템 메타 정보 필요, 관리 어려움, 속성이 다른 연관성 높은 아이템 추천 불가 (예: 불닭볶음면과 모짜렐라 치즈).
- **협업 필터링 (Collaborative Filtering, CF)**: 콘텐츠 기반 단점 보완.
    - **최근접 이웃 모델 (Neighborhood Model)**: 사용자-아이템 평점 데이터 기반.
        - **사용자 기반 (User-Based CF)**: 유사한 성향 사용자 그룹이 선호한 아이템 추천. (유사도: 피어슨, 코사인)
        - **아이템 기반 (Item-Based CF)**: 사용자가 선호한 아이템과 유사한 특성의 다른 아이템 추천.
        - 희소 행렬 (Sparse matrix) 문제: 평점 데이터 부족. 구매 이력, 클릭 등 암묵적(Implicit) 데이터 활용.
    - **잠재 요인 모델 (Latent Factor Model)**: 암묵적 데이터에 적합.
        - 원리: 사용자-아이템 행렬을 행렬 분해(Matrix Factorization)하여 숨겨진 잠재 요인(latent factor) 도출.
        - 행렬 분해 알고리즘: SVD (결측치 imputation 필요), ALS (명시적 피드백), SGD (암시적 피드백).
        - 분해된 행렬을 다시 내적하여 비어있던 암묵적 데이터 채우고 관심도 높은 아이템 추천.
    - **CF 맹점**:
        - 콜드 스타트 (Cold Start): 신규 사용자/아이템 데이터 부족.
        - 희박성 (Sparsity): 상호작용 부족 시 잠재요인 도출 어려움.
        - 특이 취향 사용자 (Gray sheep) 추천 정확도 저하.
- **하이브리드 필터링**: CF와 콘텐츠 기반 결합.
    1. 각각 구현 후 결과 혼합 (가중합 등).
    2. CF에 콘텐츠 기반 특성 적용 (사용자 특성 변수 추가 등).
    3. 콘텐츠 기반에 CF 특성 적용 (사용자 프로필 정보 차원 압축).

## 13.10 인공 신경망 (CNN, RNN, LSTM)

데이터 분석, 프로그래밍, 수리 지식 필요. 복잡한 비선형 관계 분석, 오류/잡음에 강해 일반화 성능 우수.

- **기원**: 생명체 뉴런 모방. 1940년대 시작, 빅데이터 및 GPU 발전으로 2000년대 이후 주목. 제프리 힌튼 교수가 딥러닝(Deep Learning) 기법 탄생시킴.
- **구조**: 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer). (은닉층 때문에 블랙박스 모형)
- **오류 역전파 (Backpropagation)**: 출력값과 실제값 차이를 이용해 은닉층 가중치 조절.
- **딥러닝**: 은닉층이 2개 이상인 신경망.
- **퍼셉트론 (Perceptron)**: 신경망 기본 단위. 입력값에 가중치 적용 합산 후 임계값 초과 시 1, 아니면 0 출력.
    - 다층 퍼셉트론 (Multi-Layer Perceptron, MLP): 여러 퍼셉트론 구성, 비선형 분류 가능 (XOR 게이트 구현).
- **문제 해결**: 지역 최솟값 문제 (활성화 함수, Drop-out 등으로 해결).

### 13.10.1 CNN (Convolutional Neural Network, 합성곱 신경망)

사람 시신경 구조 모방, 데이터 특징 추출하여 패턴 파악 (주로 이미지 인식).

- **문제점**: 픽셀 단위가 커지면 연산량 급증, 필기체 등 변화에 취약.
- **주요 과정**:
    - **합성곱 (Convolution)**: 입력 데이터에 필터(Filter)를 적용하여 특징맵(Feature map) 생성. (Stride: 필터 이동 간격)
    - **패딩 (Padding)**: 입력 데이터 가장자리에 특정 값(주로 0)을 채워 특징맵 크기 유지 및 데이터 손실 방지.
    - **풀링 (Pooling)**: 특징맵 사이즈 축소, 연산량 감소, 노이즈 상쇄. (Max-pooling, Average-pooling 등)
- **전체 과정**: 입력 -> (합성곱 -> 풀링) 반복 -> Flatten -> 완전연결계층(Dense) -> 출력.

### 13.10.2 RNN (Recurrent Neural Network, 순환 신경망)과 LSTM

자연어 처리 등 시계열 데이터 활용에 특화.

- **RNN**: 내부에 순환 구조, 이전 계산 결과가 다음 연산에 영향 (피드포워드와 차이).
    - 구조: 입력(xt) -> 은닉 상태(ht, 기억 담당) -> 출력(yt). ht는 $h_{t-1}$과 xt의 함수.
    - 활용: 문장 긍/부정 판단(다중입력-단일출력), 이미지 캡셔닝(단일입력-다중출력), 번역기(다중입력-다중출력), 단어 예측(다중입력-각 시점 출력).
    - 문제점: 장기 의존성 문제 (Long-term dependency) - 과거 시점 정보가 멀리 전달되지 못함.
- **LSTM (Long Short-Term Memory)**: RNN의 장기 의존성 문제 해결.
    - 셀 상태 (Cell state) 추가: 정보가 얼마나 유지될지 결정.
    - 게이트 (Gate): 입력 게이트, 망각 게이트, 출력 게이트를 통해 정보 흐름 제어 (시그모이드, 하이퍼볼릭 탄젠트 함수 사용).
- **기타**: 트랜스포머 (Transformer) 알고리즘 등 발전.

# 14. 모델 평가

## 14.1 학습 셋, 검증 셋, 테스트 셋과 과적합 해결

모델 구축 시 데이터를 학습, 검증, 테스트 셋으로 분리하여 과적합을 방지하고 일반화 성능 제고

- **학습 셋 (Training Set)**: 모델 파라미터 학습에 사용
- **검증 셋 (Validation Set)**: 학습 중인 모델의 성능을 검증하여 과적합을 방지하고, 하이퍼파라미터 튜닝에 사용
- **테스트 셋 (Test Set)**: 학습과 검증이 완료된 모델의 최종 성능을 평가
- *과적합 (Overfitting)**은 모델이 학습 데이터에만 지나치게 최적화되어 새로운 데이터에 대한 예측력이 떨어지는 현상이며, **과소적합 (Underfitting)**은 학습이 부족하여 데이터의 패턴을 제대로 잡아내지 못하는 현상

**과적합 해결 주요 방법**:

- 데이터 추가 확보
- 결측값 및 이상치 처리
- 변수 선택 및 차원 축소 (유의도 낮은 변수 제거 등)
- 정규화 (Regularization): 모델 복잡도에 페널티를 부여 (예: Ridge, Lasso).
- 드롭아웃 (Dropout): 신경망 학습 시 일부 뉴런을 임의로 제외.

## 14.2 주요 교차 검증 방법

데이터를 효율적으로 활용하여 모델 성능을 안정적으로 평가하고 과적합을 줄입니다.

- **k-Fold Cross Validation (k-겹 교차 검증)**: 데이터를 k개의 부분집합(fold)으로 나누어, k-1개는 학습에, 1개는 검증에 사용하는 과정을 k번 반복합니다. 모델 성능은 k개 검증 결과의 평균으로 평가
- **LOOCV (Leave-One-Out Cross-Validation)**: k-Fold에서 k를 전체 데이터 수로 설정. 즉, 하나의 샘플만 검증용으로 사용하고 나머지는 학습용으로 사용하는 과정을 반복합니다. 편향은 적으나 분산이 크고 계산 비용이 높음
- **Stratified K-Fold Cross Validation (층화 K-겹 교차 검증)**: 각 fold의 클래스(범주) 비율이 전체 데이터셋의 클래스 비율과 유사하도록 데이터를 분할합니다. 주로 분류 모델에서 클래스 불균형 시 사용
- **Nested Cross Validation (중첩 교차 검증)**: 외부 루프에서 학습/테스트 분할, 내부 루프에서 학습/검증 분할(하이퍼파라미터 튜닝)을 수행하여 모델 일반화 성능을 더 엄격하게 평가
- **Grid Search Cross Validation (격자 탐색 교차 검증)**: 지정된 하이퍼파라미터 값들의 모든 조합에 대해 교차 검증을 수행하여 최적의 조합을 탐ㅅㅐㄱ

## 14.3 회귀 성능 평가 지표

- R2 **(결정계수) 및 Adjusted** R2 **(조정된 결정계수)**: 모델이 종속변수의 분산을 얼마나 설명하는지를 나타냅니다. Adj R2는 독립변수 개수를 보정
- **RMSE (Root Mean Square Error, 평균 제곱근 오차)**: 예측값과 실제값 차이의 제곱 평균에 루트를 씌운 값. 오차의 크기를 직접적으로 나타내며, 이상치에 민감
- **MAE (Mean Absolute Error, 평균 절대 오차)**: 예측값과 실제값 차이의 절댓값 평균. RMSE보다 이상치에 덜 민감
- **MAPE (Mean Absolute Percentage Error, 평균 절대 백분율 오차)**: MAE를 백분율로 변환한 값. 상대적 오차를 나타내어 스케일이 다른 모델 간 비교에 용이하나, 실제값이 0이거나 0에 가까우면 문제 발생 소지
- **RMSLE (Root Mean Square Logarithmic Error, 평균 제곱근 로그 오차)**: RMSE 계산 시 값에 로그를 취합니다. 상대적 오차를 반영하며, 과소예측에 더 큰 페널티
- **AIC (Akaike's Information Criterion) 및 BIC (Bayesian Information Criterion)**: 모델의 적합도와 복잡도(변수 개수)를 함께 고려하는 지표. 값이 작을수록 좋습니다. BIC가 변수 개수에 더 큰 페널티를 부여

## 14.4 분류, 추천 성능 평가 지표

### 14.4.1 혼동 행렬 (Confusion Matrix)

모델의 예측 결과를 실제 값과 비교한 표. (TP, FP, FN, TN)

### 14.4.2 정확도, 오분류율, 정밀도, 민감도(재현율), 특이도, F1-Score

- **정확도 (Accuracy)**: 전체 중 올바르게 분류한 비율. (클래스 불균형 시 주의)
- **오분류율 (Error Rate)**: 전체 중 잘못 분류한 비율
- **정밀도 (Precision)**: Positive로 예측한 것 중 실제 Positive인 비율
- **민감도 (Sensitivity) / 재현율 (Recall)**: 실제 Positive 중 Positive로 예측한 비율
- **특이도 (Specificity)**: 실제 Negative 중 Negative로 예측한 비율
- **F1-Score**: 정밀도와 재현율의 조화 평균. (클래스 불균형에 유용)

### 14.4.3 향상도 (Lift) 및 관련 차트

모델이 무작위 선택보다 얼마나 더 잘 예측하는지 나타냅니다. 향상도 테이블, 향상도 차트, 누적 향상도 곡선 등을 통해 모델 성능을 평가하고 임계값을 설정하는 데 활용합니다.

### 14.4.4 ROC 곡선과 AUC

- **ROC (Receiver Operating Characteristic) 곡선**: 분류 임계값 변화에 따른 TPR(민감도)과 FPR(1-특이도)의 관계를 시각화. 좌측 상단에 가까울수록 좋은 모델
- **AUC (Area Under the ROC Curve)**: ROC 곡선 아래 면적. 1에 가까울수록 성능이 우수

### 14.4.5 수익 곡선 (Profit Curve)

오분류 비용과 정분류 이익을 고려하여 모델의 비즈니스적 가치를 평가하고, 최적의 임계값을 찾는 데 도움

### 14.4.6 Precision@k, Recall@k, MAP (추천 시스템)

- **Precision@k, Recall@k**: 상위 k개 추천 항목에 대한 정밀도와 재현율
- **MAP (Mean Average Precision)**: 모든 사용자의 Average Precision 평균. 추천 순서 고려한 지표

## 14.5 A/B 테스트와 MAB (Multi-Armed Bandit)

### 14.5.1 A/B 테스트

두 가지 이상의 대안을 비교하여 더 나은 성과를 내는 것을 선택하는 실험 방법

- **주의사항**: 무작위 집단 배분, 충분한 표본 크기 및 기간, 한 번에 한 가지 요소 변경
- **단점**: 시간과 비용 소요, 최적안 탐색의 비효율성

### 14.5.2 MAB (Multi-Armed Bandit)

탐색(Exploration)과 활용(Exploitation)의 균형을 통해 제한된 상황에서 보상을 최대화하는 전략

예: ϵ-greedy, UCB, Thompson Sampling) A/B 테스트의 단점을 보완

## 14.6 유의확률의 함정

P-value : 귀무가설이 맞다는 가정 하에 관찰된 결과 또는 그보다 더 극단적인 결과가 나타날 확률

- **주의점**: 표본 크기에 영향, 0.05 기준의 임의성, 실제 중요성과 다름
- **권장**: 효과 크기, 신뢰구간 등 다양한 지표와 함께 종합적으로 해석해야

## Study Schedule

| 주차 | 공부 범위 | 완료 여부 |
| --- | --- | --- |
| 1주차 | 1부 p.2~56 | ✅ |
| 2주차 | 1부 p.57~79 | ✅ |
| 3주차 | 2부 p.82~120 | ✅ |
| 4주차 | 2부 p.121~202 | ✅ |
| 5주차 | 2부 p.203~254 | ✅ |
| 6주차 | 3부 p.300~356 | ✅ |
| 7주차 | 3부 p.357~615 | ✅ |

# 확인 문제

## **문제 1. 선형 회귀**

> 🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.
> 
> 
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요.
> 
> **ŷ = 33.73 + 0.516X**
> 
> - **X**: 아버지의 키 (cm)
> - **ŷ**: 아들의 예상 키 (cm)

```
아버지의 키와 아들의 키 사이에는 양의 선형 관계가 있으며, 아버지의 키가 클수록 아들의 키도 커지는 경향이 있지만, 그 증가폭은 아버지 키 증가분의 약 절반 정도(0.516배)
```

---

## **문제 2. 다항 회귀**

> 🧚 선형 회귀와 비교하여 다항 회귀는 언제 사용할 수 있나요?
> 

```
독립변수와 종속변수 간의 관계가 직선이 아닌 곡선 형태를 나타낼 때
```

---

## **문제 3. 로지스틱 회귀**

> 🧚 푸앙대학에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 ‘낮음’ 입니다.
> 

| 변수 | Odds Ratio Estimates | 95% Wald Confidence Limits |
| --- | --- | --- |
| Study Hours | **2.34** | (1.89, 2.88) |
| Attendance Rate | **3.87** | (2.92, 5.13) |

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.
> 

```
주당 공부 시간(Study Hours)의 오즈비 2.34: 다른 조건이 동일할 때, 주당 공부 시간이 1단위 증가하면 학업 성취도가 '낮음' 범주에 비해 다른 범주('보통' 또는 '높음')에 속할 확률이 2.34배 높아진다는 의미
출석률(Attendance Rate)의 오즈비 3.87: 다른 조건이 동일할 때, 출석률이 1단위 증가하면 학업 성취도가 '낮음' 범주에 비해 다른 범주('보통' 또는 '높음')에 속할 확률이 3.87배 높아진다는 의미
```

> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.
> 

```
추정된 오즈비의 불확실성 정도. 반복적인 표본 추출을 통해 같은 분석을 수행했을 때, 실제 오즈비가 해당 구간 내에 존재할 것이라고 95% 신뢰할 수 있다는 의미
```

> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
> 

```
공부 시간과 출석률 모두 학업 성취도에 긍정적인 영향을 미치므로, 이 두 요소를 함께 향상시킬 수 있는 통합적인 프로그램을 개발하는 것이 중요
```

---

## **문제 4. k-means 클러스터링**

> 🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.
> 

```
여러 k 값에 대해 평균 실루엣 계수를 계산하여, 이 값이 가장 높은 k를 최적의 군집 수로 선택
```

### 🎉 수고하셨습니다.